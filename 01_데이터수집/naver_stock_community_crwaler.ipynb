{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f32afd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium\n",
      "  Downloading selenium-4.33.0-py3-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in /mnt/d/Work_Dev/AI-Dev/linux/py/.venv/lib/python3.12/site-packages (4.13.4)\n",
      "Requirement already satisfied: requests in /mnt/d/Work_Dev/AI-Dev/linux/py/.venv/lib/python3.12/site-packages (2.32.4)\n",
      "Requirement already satisfied: urllib3~=2.4.0 in /mnt/d/Work_Dev/AI-Dev/linux/py/.venv/lib/python3.12/site-packages (from urllib3[socks]~=2.4.0->selenium) (2.4.0)\n",
      "Collecting trio~=0.30.0 (from selenium)\n",
      "  Downloading trio-0.30.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting trio-websocket~=0.12.2 (from selenium)\n",
      "  Downloading trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: certifi>=2025.4.26 in /mnt/d/Work_Dev/AI-Dev/linux/py/.venv/lib/python3.12/site-packages (from selenium) (2025.4.26)\n",
      "Collecting typing_extensions~=4.13.2 (from selenium)\n",
      "  Downloading typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: websocket-client~=1.8.0 in /mnt/d/Work_Dev/AI-Dev/linux/py/.venv/lib/python3.12/site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in /mnt/d/Work_Dev/AI-Dev/linux/py/.venv/lib/python3.12/site-packages (from trio~=0.30.0->selenium) (25.3.0)\n",
      "Collecting sortedcontainers (from trio~=0.30.0->selenium)\n",
      "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: idna in /mnt/d/Work_Dev/AI-Dev/linux/py/.venv/lib/python3.12/site-packages (from trio~=0.30.0->selenium) (3.10)\n",
      "Collecting outcome (from trio~=0.30.0->selenium)\n",
      "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in /mnt/d/Work_Dev/AI-Dev/linux/py/.venv/lib/python3.12/site-packages (from trio~=0.30.0->selenium) (1.3.1)\n",
      "Collecting wsproto>=0.14 (from trio-websocket~=0.12.2->selenium)\n",
      "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /mnt/d/Work_Dev/AI-Dev/linux/py/.venv/lib/python3.12/site-packages (from urllib3[socks]~=2.4.0->selenium) (1.7.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /mnt/d/Work_Dev/AI-Dev/linux/py/.venv/lib/python3.12/site-packages (from beautifulsoup4) (2.7)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /mnt/d/Work_Dev/AI-Dev/linux/py/.venv/lib/python3.12/site-packages (from requests) (3.4.2)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in /mnt/d/Work_Dev/AI-Dev/linux/py/.venv/lib/python3.12/site-packages (from wsproto>=0.14->trio-websocket~=0.12.2->selenium) (0.16.0)\n",
      "Downloading selenium-4.33.0-py3-none-any.whl (9.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading trio-0.30.0-py3-none-any.whl (499 kB)\n",
      "Downloading trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
      "Downloading typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
      "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
      "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Installing collected packages: sortedcontainers, wsproto, typing_extensions, outcome, trio, trio-websocket, selenium\n",
      "\u001b[2K  Attempting uninstall: typing_extensions━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/7\u001b[0m [wsproto]\n",
      "\u001b[2K    Found existing installation: typing_extensions 4.14.0━━━━━\u001b[0m \u001b[32m1/7\u001b[0m [wsproto]\n",
      "\u001b[2K    Uninstalling typing_extensions-4.14.0:━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/7\u001b[0m [wsproto]\n",
      "\u001b[2K      Successfully uninstalled typing_extensions-4.14.0━━━━━━━━━━━\u001b[0m \u001b[32m2/7\u001b[0m [typing_extensions]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7/7\u001b[0m [selenium]6/7\u001b[0m [selenium]ocket]ns]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed outcome-1.3.0.post0 selenium-4.33.0 sortedcontainers-2.4.0 trio-0.30.0 trio-websocket-0.12.2 typing_extensions-4.13.2 wsproto-1.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium beautifulsoup4 requests pandas tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "97c8aae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 추출된 종목 코드 ---\n",
      "종목명: 카카오페이, 코드: 377300\n",
      "종목명: 두산에너빌리티, 코드: 034020\n",
      "종목명: 삼성전자, 코드: 005930\n",
      "종목명: 카카오, 코드: 035720\n",
      "종목명: NAVER, 코드: 035420\n",
      "종목명: 한국전력, 코드: 015760\n",
      "종목명: SK하이닉스, 코드: 000660\n",
      "종목명: 한화오션, 코드: 042660\n",
      "종목명: 현대차, 코드: 005380\n",
      "종목명: LG씨엔에스, 코드: 064400\n",
      "종목명: 제주반도체, 코드: 080220\n",
      "종목명: 현대건설, 코드: 000720\n",
      "종목명: 풍산, 코드: 103140\n",
      "종목명: 카카오뱅크, 코드: 323410\n",
      "종목명: 현대로템, 코드: 064350\n",
      "종목명: 한화시스템, 코드: 272210\n",
      "종목명: 파미셀, 코드: 005690\n",
      "종목명: SK이터닉스, 코드: 475150\n",
      "종목명: 삼성중공업, 코드: 010140\n",
      "종목명: 한미반도체, 코드: 042700\n",
      "종목명: 기아, 코드: 000270\n",
      "종목명: HD한국조선해양, 코드: 009540\n",
      "종목명: 에코프로비엠, 코드: 247540\n",
      "종목명: 동방, 코드: 004140\n",
      "종목명: POSCO홀딩스, 코드: 005490\n",
      "종목명: 한화에어로스페이스, 코드: 012450\n",
      "종목명: 한화솔루션, 코드: 009830\n",
      "종목명: HD현대중공업, 코드: 329180\n",
      "종목명: 레인보우로보틱스, 코드: 277810\n",
      "종목명: 케이옥션, 코드: 102370\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "전체 종목 크롤링 진행률: 100%|██████████| 30/30 [03:18<00:00,  6.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 모든 종목의 게시글 데이터 ---\n",
      "                 날짜                 제목       닉네임      종목        게시글  댓글수  조회수  \\\n",
      "0  2025.06.25 14:17     개미들이 돈이 너무 많다.  2bco****  377300  305073543    0    1   \n",
      "1  2025.06.25 14:17  스테이블코인 도입시 최고 수혜주  h2******  377300  305073535    0    5   \n",
      "2  2025.06.25 14:15                  ?  y903****  377300  305073301    0   49   \n",
      "3  2025.06.25 14:14     내일 정지야 왜 매수하지마  ch******  377300  305073230    0  112   \n",
      "4  2025.06.25 14:14           내일 더 오른다  good****  377300  305073224    0   66   \n",
      "\n",
      "   공감  비공감  \n",
      "0   0    0  \n",
      "1   0    0  \n",
      "2   0    0  \n",
      "3   0    1  \n",
      "4   2    0  \n",
      "총 2856개의 게시글을 수집했습니다.\n",
      "저장 완료: data/articleList_20250625_141719.csv\n",
      "WebDriver가 종료되었습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import datetime \n",
    "from bs4 import BeautifulSoup as bs\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "from tqdm import trange, tqdm # 진행률 표시줄\n",
    "\n",
    "# Selenium WebDriver 인스턴스를 전역적으로 관리하거나, 각 함수에서 인자로 전달하도록 변경\n",
    "# driver = None # 초기화 시점에 None으로 설정\n",
    "# headers는 BeautifulSoup와 requests에 사용되는 HTTP 헤더\n",
    "headers = {\"user-agent\": \"Mozilla/5.0\"}\n",
    "current_date = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "filepath = f'data/articleList_{current_date}.csv'\n",
    "\n",
    "last_page_default = 5 # 기본값 설정, 실제로는 get_last_page 함수에서 동적으로 결정됨\n",
    "\n",
    "def initialize_driver():\n",
    "    \"\"\"Selenium WebDriver를 초기화하고 반환합니다.\"\"\"\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--headless')  # 브라우저 창을 띄우지 않고 실행 (백그라운드 실행)\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "    options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')\n",
    "    options.add_argument('lang=ko_KR') # 한국어 설정\n",
    "    \n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    driver.set_page_load_timeout(30) # 페이지 로딩 최대 30초 대기\n",
    "    return driver\n",
    "\n",
    "def get_item_code_list():\n",
    "    \"\"\"네이버 금융 인기 검색 종목 코드를 가져옵니다.\"\"\"\n",
    "    url = \"https://finance.naver.com/sise/lastsearch2.naver\"\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = bs(response.text, 'html.parser') \n",
    "    item_codes = []\n",
    "    \n",
    "    # 'div.box_type_l' 안에 있는 'table.type_5'를 찾습니다.\n",
    "    # select_one은 찾지 못하면 None을 반환하므로 예외 처리\n",
    "    list_table_container = soup.select_one('div.box_type_l')\n",
    "    if not list_table_container:\n",
    "        print(\"종목 코드 테이블 컨테이너를 찾을 수 없습니다.\")\n",
    "        return []\n",
    "\n",
    "    list_table = list_table_container.select_one('table.type_5')\n",
    "    if not list_table:\n",
    "        print(\"종목 코드 테이블을 찾을 수 없습니다.\")\n",
    "        return []\n",
    "\n",
    "    code_pattern = r'code=(\\d+)'\n",
    "\n",
    "    print(\"--- 추출된 종목 코드 ---\")\n",
    "    for a_tag in list_table.find_all('a'):\n",
    "        href_value = a_tag.get('href')\n",
    "        \n",
    "        if href_value:\n",
    "            match = re.search(code_pattern, href_value)\n",
    "            if match:\n",
    "                stock_code = match.group(1)\n",
    "                stock_name = a_tag.text.strip()\n",
    "                print(f\"종목명: {stock_name}, 코드: {stock_code}\")\n",
    "                item_codes.append(stock_code)\n",
    "    return item_codes\n",
    "\n",
    "def get_item_url(item_code, page_no=1):\n",
    "    \"\"\"주어진 종목 코드와 페이지 번호에 대한 게시판 URL을 생성합니다.\"\"\"\n",
    "    return f\"https://finance.naver.com/item/board.naver?code={item_code}&page={page_no}\"\n",
    "\n",
    "def get_last_page(driver, item_code):\n",
    "    \"\"\"\n",
    "    Selenium을 사용하여 특정 종목의 마지막 페이지 번호를 가져옵니다.\n",
    "    \"\"\"\n",
    "    url = get_item_url(item_code)\n",
    "    driver.get(url)\n",
    "\n",
    "    try:\n",
    "        # 'td.pgRR' 클래스를 가진 요소를 찾을 때까지 최대 10초 대기\n",
    "        last_page_element = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, 'td.pgRR a'))\n",
    "        )\n",
    "        \n",
    "        href_value = last_page_element.get_attribute('href')\n",
    "        \n",
    "        # href 값에서 'page=' 다음의 숫자를 추출\n",
    "        match = re.search(r'page=(\\d+)', href_value)\n",
    "        if match:\n",
    "            return int(match.group(1))\n",
    "        else:\n",
    "            print(f\"경고: 종목 {item_code}의 마지막 페이지 번호를 추출할 수 없습니다. 기본값 사용.\")\n",
    "            return last_page_default # 추출 실패 시 적절한 기본값\n",
    "    except TimeoutException:\n",
    "        print(f\"경고: 종목 {item_code}의 마지막 페이지 요소를 찾지 못했습니다 (타임아웃). 기본값 사용.\")\n",
    "        return last_page_default # 타임아웃 발생 시 기본값\n",
    "    except NoSuchElementException:\n",
    "        print(f\"경고: 종목 {item_code}의 마지막 페이지 요소를 찾을 수 없습니다. 기본값 사용.\")\n",
    "        return last_page_default # 요소가 없을 시 기본값\n",
    "    except Exception as e:\n",
    "        print(f\"오류: 종목 {item_code}의 마지막 페이지를 가져오는 중 예상치 못한 오류 발생: {e}. 기본값 사용.\")\n",
    "        return last_page_default\n",
    "\n",
    "def get_one_page(driver, item_code, page_no):\n",
    "    \"\"\"\n",
    "    Selenium을 사용하여 특정 종목의 한 페이지 게시글 정보를 가져옵니다.\n",
    "    동적으로 로드되는 내용을 처리합니다.\n",
    "    \"\"\"\n",
    "    page_url = get_item_url(item_code, page_no)\n",
    "    #print(f\"크롤링 중: 종목 {item_code}, 페이지 {page_no} ({page_url})\")\n",
    "    driver.get(page_url)\n",
    "\n",
    "    dataframe = pd.DataFrame(columns=['날짜', '제목', '닉네임', '종목', '게시글', '댓글수', '조회수', '공감', '비공감'])\n",
    "    \n",
    "    # 게시글 목록이 로드될 때까지 대기\n",
    "    # 여기서는 게시글의 첫 번째 컬럼(날짜)이 나타날 때까지 기다립니다.\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, 'span.tah.p10.gray03'))\n",
    "        )\n",
    "    except TimeoutException:\n",
    "        print(f\"경고: 종목 {item_code}, {page_no} 페이지의 게시글 목록 로딩 타임아웃.\")\n",
    "        return dataframe # 데이터프레임 반환 후 다음 페이지로 이동\n",
    "    except Exception as e:\n",
    "        print(f\"오류: 종목 {item_code}, {page_no} 페이지 로딩 중 예상치 못한 오류 발생: {e}\")\n",
    "        return dataframe\n",
    "\n",
    "    # BeautifulSoup으로 페이지 소스 파싱 (Selenium이 로드한 최종 HTML)\n",
    "    soup =bs(driver.page_source, 'html.parser')\n",
    "\n",
    "    posts = soup.find_all('tr')\n",
    "    date_pattern = r'\\d{4}\\.\\d{2}\\.\\d{2} \\d{2}:\\d{2}'\n",
    "    comment_count_pattern = r'\\[(\\d+)\\]'\n",
    "    article_number_pattern = r'nid=(\\d+)'\n",
    "    cleanbot_str = '클린봇이 이용자 보호를 위해 숨긴 게시글입니다.'\n",
    "\n",
    "    for post in posts:\n",
    "        tds = post.find_all('td')\n",
    "        # 유효한 게시글 행인지 확인 (td 개수, 날짜 형식 등)\n",
    "        if not tds or len(tds) < 6:\n",
    "            continue\n",
    "        \n",
    "        date_span = post.find('span', class_='tah p10 gray03')\n",
    "        # 날짜가 유효한 형식인지 확인\n",
    "        if date_span:\n",
    "            date_text = date_span.text.strip()\n",
    "            if not re.fullmatch(date_pattern, date_text): # 정확히 날짜 패턴에 맞는지 확인\n",
    "                continue # 날짜 형식이 아니면 게시글 아님\n",
    "            date = date_text\n",
    "        else:\n",
    "            continue # 날짜 스팬이 없으면 게시글 아님\n",
    "\n",
    "        # 제목, 링크, 닉네임, 조회수, 공감, 비공감 추출\n",
    "        title_td = tds[1]\n",
    "        link_tag = title_td.find('a')\n",
    "        \n",
    "        if not link_tag: # 링크가 없으면 게시글 아님\n",
    "            continue\n",
    "\n",
    "        link = link_tag['href'] if 'href' in link_tag.attrs else ''\n",
    "        if not link: # href 속성 값이 없으면 건너뛰기\n",
    "            continue\n",
    "\n",
    "        article_number_match = re.search(article_number_pattern, link)\n",
    "        article_number = article_number_match.group(1) if article_number_match else ''\n",
    "        if not article_number: # 게시글 번호가 없으면 건너뛰기\n",
    "            continue\n",
    "\n",
    "        full_title_text = title_td.text.strip()\n",
    "        comment_count_match = re.search(comment_count_pattern, full_title_text)\n",
    "        comment_count = comment_count_match.group(1) if comment_count_match else 0\n",
    "        \n",
    "        # 댓글 수를 제외한 제목 추출 (정규표현식으로 더 정확하게)\n",
    "        title = re.sub(comment_count_pattern, '', full_title_text).strip()\n",
    "        title = re.sub(r'\\s+', ' ', title).strip() # 여러 공백 하나로 줄이기\n",
    "\n",
    "        # '클린봇' 게시글 필터링\n",
    "        if cleanbot_str in title:\n",
    "            continue\n",
    "        \n",
    "        # 나머지 정보 추출 (존재하지 않을 수 있으므로 인덱스 체크)\n",
    "        nickname = tds[2].text.strip() if len(tds) > 2 else ''\n",
    "        views = tds[3].text.strip() if len(tds) > 3 else ''\n",
    "        likes = tds[4].text.strip() if len(tds) > 4 else ''\n",
    "        dislikes = tds[5].text.strip() if len(tds) > 5 else ''\n",
    "\n",
    "        # DataFrame에 추가\n",
    "        new_row = pd.DataFrame([{\n",
    "            '날짜': date,\n",
    "            '제목': title,\n",
    "            '닉네임': nickname,\n",
    "            '종목': item_code,\n",
    "            '게시글': article_number,\n",
    "            '댓글수': comment_count,\n",
    "            '조회수': views,\n",
    "            '공감': likes,\n",
    "            '비공감': dislikes\n",
    "        }])\n",
    "        dataframe = pd.concat([dataframe, new_row], ignore_index=True)\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def get_all_pages(driver, item_code):\n",
    "    \"\"\"\n",
    "    특정 종목의 모든 게시판 페이지를 크롤링합니다.\n",
    "    \"\"\"\n",
    "    last_page = get_last_page(driver, item_code)\n",
    "    \n",
    "    # 디버깅을 위해 last_page 제한 \n",
    "    if last_page > last_page_default: last_page = last_page_default\n",
    "    \n",
    "    # print(f\"\\n종목 코드: {item_code}, 총 {last_page} 페이지를 크롤링합니다.\")\n",
    "    \n",
    "    page_list = []\n",
    "    # 1페이지부터 마지막 페이지까지 반복\n",
    "    for page_num in range(1, last_page + 1):\n",
    "        df_one_page = get_one_page(driver, item_code, page_num)\n",
    "        if not df_one_page.empty: # 빈 데이터프레임이 아니면 추가\n",
    "            page_list.append(df_one_page)\n",
    "        # 과도한 요청 방지를 위해 페이지당 딜레이 추가\n",
    "        time.sleep(0.5) \n",
    "    \n",
    "    if not page_list:\n",
    "        print(f\"경고: 종목 {item_code}에서 어떤 게시글도 가져오지 못했습니다.\")\n",
    "        return pd.DataFrame(columns=['날짜', '제목', '닉네임', '종목', '게시글', '댓글수', '조회수', '공감', '비공감'])\n",
    "\n",
    "    df_all_page = pd.concat(page_list, ignore_index=True)\n",
    "    \n",
    "    # 데이터 타입 변환\n",
    "    for col in ['댓글수', '조회수', '공감', '비공감']:\n",
    "        # 숫자만 포함하는지 확인 후 int로 변환, 실패 시 0으로 설정\n",
    "        df_all_page[col] = pd.to_numeric(df_all_page[col], errors='coerce').fillna(0).astype(int)\n",
    "    \n",
    "    return df_all_page\n",
    "            \n",
    "if __name__ == \"__main__\":\n",
    "    driver = None # 드라이버 변수 선언\n",
    "    try:\n",
    "        driver = initialize_driver() # 드라이버 초기화\n",
    "        \n",
    "        df_list = []\n",
    "        item_code_list = get_item_code_list()\n",
    "        \n",
    "        # 실제 크롤링 시에는 전체 item_code_list를 사용합니다.\n",
    "        # 테스트를 위해 일부 종목만 크롤링하려면 슬라이싱을 사용하세요.\n",
    "        # 예: item_code_list = item_code_list[:3] \n",
    "        \n",
    "        # item_code_list의 각 종목 코드를 순회\n",
    "        for item_code in trange(len(item_code_list), desc=\"전체 종목 크롤링 진행률\"):\n",
    "            current_item_code = item_code_list[item_code] # 리스트에서 실제 종목 코드를 가져옴\n",
    "            df = get_all_pages(driver, current_item_code) # 드라이버 인스턴스 전달\n",
    "            if not df.empty:\n",
    "                df_list.append(df)\n",
    "            time.sleep(1) # 각 종목 크롤링 후 대기\n",
    "        \n",
    "        if not df_list:\n",
    "            print(\"모든 종목에서 데이터를 가져오는 데 실패했습니다.\")\n",
    "            df_all = pd.DataFrame()\n",
    "        else:\n",
    "            # 모든 종목의 데이터프레임을 하나로 합치기\n",
    "            df_all = pd.concat(df_list, ignore_index=True)\n",
    "            print(\"\\n--- 모든 종목의 게시글 데이터 ---\")\n",
    "            print(df_all.head())\n",
    "            print(f\"총 {len(df_all)}개의 게시글을 수집했습니다.\")\n",
    "\n",
    "        # data 디렉토리 생성\n",
    "        if not os.path.exists('data'):\n",
    "            os.makedirs('data')\n",
    "        \n",
    "        # df_all을 csv로 저장\n",
    "        if not df_all.empty:\n",
    "            df_all.to_csv(filepath, index=False, encoding='utf-8-sig') # 한글 깨짐 방지를 위해 encoding 추가\n",
    "            print(f\"저장 완료: {filepath}\")\n",
    "        else:\n",
    "            print(\"수집된 데이터가 없어 CSV 파일을 저장하지 않습니다.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"크롤링 과정 중 심각한 오류 발생: {e}\")\n",
    "    finally:\n",
    "        if driver:\n",
    "            driver.quit() # 드라이버 종료\n",
    "            print(\"WebDriver가 종료되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a089c45d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 2856개의 게시글 내용을 크롤링합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "게시글 내용 및 댓글 크롤링 중: 100%|██████████| 2856/2856 [52:12<00:00,  1.10s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "경고: 총 107개의 게시글에서 오류가 발생했습니다. 일부 게시글 내용이나 댓글을 가져오지 못했을 수 있습니다.\n",
      "최종 데이터 저장 완료: data/contentsList_20250625_151254.csv\n",
      "\n",
      "--- 합쳐진 데이터프레임의 상위 5개 행 ---\n",
      "                 날짜                 제목       닉네임      종목        게시글  댓글수  조회수  \\\n",
      "0  2025.06.25 14:17     개미들이 돈이 너무 많다.  2bco****  377300  305073543    0    1   \n",
      "1  2025.06.25 14:17  스테이블코인 도입시 최고 수혜주  h2******  377300  305073535    0    5   \n",
      "2  2025.06.25 14:15                  ?  y903****  377300  305073301    0   49   \n",
      "3  2025.06.25 14:14     내일 정지야 왜 매수하지마  ch******  377300  305073230    0  112   \n",
      "4  2025.06.25 14:14           내일 더 오른다  good****  377300  305073224    0   66   \n",
      "\n",
      "   공감  비공감                                              게시글내용         댓글  \n",
      "0   0    0                                   돈이 참 많다.\\n\\n\\n허허  세력이 개인이니까  \n",
      "1   0    0  스테이블코인 결제 시스템이 국내에 본격적으로 도입될 경우, 가장 큰 수혜를 입을 수...             \n",
      "2   0    0                                          기관 10 자녀들             \n",
      "3   0    1                                               큰일나요             \n",
      "4   2    0                                  쌀때 사둬\\n대세를 거를순 없지             \n"
     ]
    }
   ],
   "source": [
    "def get_article_content(article_list):\n",
    "    # 결과를 저장할 DataFrame 초기화\n",
    "    content_df = pd.DataFrame(columns=['게시글', '게시글내용', '댓글']) \n",
    "    \n",
    "    driver = initialize_driver() # 드라이버 초기화\n",
    "    driver.set_page_load_timeout(30) # 페이지 로딩 최대 30초 대기\n",
    "\n",
    "    error_count = 0\n",
    "    for article in tqdm(article_list, desc=\"게시글 내용 및 댓글 크롤링 중\"):\n",
    "        stock_code, article_id = article[0], article[1]\n",
    "        \n",
    "        # 게시글 URL 생성\n",
    "        url = f'https://finance.naver.com/item/board_read.naver?code={stock_code}&nid={article_id}'\n",
    "        \n",
    "        try:\n",
    "            # WebDriver를 통해 URL 접속\n",
    "            driver.get(url)\n",
    "\n",
    "            # div#body가 나타날 때까지 페이지 로딩을 기다림 (최대 10초)\n",
    "            WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, 'div#body'))\n",
    "            )\n",
    "\n",
    "            # 게시글 내용 추출\n",
    "            content_element = driver.find_element(By.CSS_SELECTOR, 'div#body')\n",
    "            content = content_element.text.strip() if content_element else ''\n",
    "            if not content:\n",
    "                print(f\"경고: 게시글 {article_id}의 내용을 찾을 수 없습니다.\")\n",
    "            \n",
    "            # 댓글 추출\n",
    "            comments = []\n",
    "            # 댓글 요소가 로드될 때까지 잠시 대기 (optional, 하지만 필요할 수 있음)\n",
    "            time.sleep(0.5) \n",
    "            comment_elements = driver.find_elements(By.CSS_SELECTOR, 'span.u_cbox_contents')\n",
    "            for comment_elem in comment_elements:\n",
    "                comments.append(comment_elem.text.strip())\n",
    "            \n",
    "            # 댓글 리스트를 하나의 문자열로 합치기 (혹은 리스트 그대로 저장)\n",
    "            comments_str = \" | \".join(comments) if comments else \"\"\n",
    "\n",
    "        except TimeoutException:\n",
    "            content = ''\n",
    "            comments_str = ''\n",
    "            error_count += 1\n",
    "            #print(f\"경고: 게시글 {article_id} 로딩 타임아웃 발생. 내용 및 댓글을 가져올 수 없습니다.\")\n",
    "        except NoSuchElementException:\n",
    "            content = ''\n",
    "            comments_str = ''\n",
    "            error_count += 1\n",
    "            #print(f\"경고: 게시글 {article_id}에서 필요한 요소를 찾을 수 없습니다. 내용 및 댓글을 가져올 수 없습니다.\")\n",
    "        except Exception as e:\n",
    "            content = ''\n",
    "            comments_str = ''\n",
    "            error_count += 1\n",
    "            #print(f\"오류: 게시글 {article_id} 처리 중 예상치 못한 오류 발생: 내용 및 댓글을 가져올 수 없습니다.\")\n",
    "\n",
    "        # DataFrame에 결과 추가\n",
    "        new_row = pd.DataFrame([{'게시글': article_id, '게시글내용': content, '댓글': comments_str}]) # comments_str로 변경\n",
    "        content_df = pd.concat([content_df, new_row], ignore_index=True)\n",
    "        \n",
    "        # 각 게시글 처리 후 잠시 대기\n",
    "        time.sleep(0.1) \n",
    "\n",
    "    # WebDriver 종료\n",
    "    driver.quit()\n",
    "    if error_count > 0:\n",
    "        print(f\"경고: 총 {error_count}개의 게시글에서 오류가 발생했습니다. 일부 게시글 내용이나 댓글을 가져오지 못했을 수 있습니다.\")\n",
    "    return content_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # CSV 파일 불러오기: '종목'과 '게시글' 컬럼을 문자열로 지정\n",
    "    df_all = pd.read_csv(filepath, encoding='utf-8-sig', dtype={'종목': str, '게시글': str})\n",
    "    \n",
    "    # 필요한 '종목'과 '게시글' 정보만 추출하여 list of lists 형태로 변환\n",
    "    article_list_for_content = df_all[['종목', '게시글']].values.tolist()\n",
    "    \n",
    "    print(f\"총 {len(article_list_for_content)}개의 게시글 내용을 크롤링합니다.\")\n",
    "    \n",
    "    # 게시글 내용 및 댓글 크롤링 함수 호출\n",
    "    df_article = get_article_content(article_list_for_content)\n",
    "    \n",
    "    # 두 데이터프레임을 '게시글' 컬럼을 기준으로 합치기\n",
    "    merged_df = pd.merge(df_all, df_article, on='게시글', how='left')\n",
    "    \n",
    "    # data 디렉토리 생성 (이미 존재하면 건너김)\n",
    "    os.makedirs('data', exist_ok=True)\n",
    "    \n",
    "    # 합쳐진 DataFrame을 CSV로 저장\n",
    "    current_date = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    contentfilepath = f'data/naver_stock_community_{current_date}.csv'\n",
    "    merged_df.to_csv(contentfilepath, index=False, encoding='utf-8-sig') # 한글 깨짐 방지를 위해 encoding 추가\n",
    "    \n",
    "    print(f\"최종 데이터 저장 완료: {contentfilepath}\")\n",
    "    print(\"\\n--- 합쳐진 데이터프레임의 상위 5개 행 ---\")\n",
    "    print(merged_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
